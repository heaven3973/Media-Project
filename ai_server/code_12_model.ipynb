{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0: ì¢…ì´  \n",
    "1: ì¢…ì´íŒ©  \n",
    "2: ì¢…ì´ì»µ  \n",
    "3: ìº”ë¥˜  \n",
    "4: ìœ ë¦¬ë³‘  \n",
    "5: í˜íŠ¸  \n",
    "6: í”Œë¼ìŠ¤í‹±  \n",
    "7: ë¹„ë‹  \n",
    "8: ìœ ë¦¬ + ë‹¤ì¤‘í¬ì¥ì¬  \n",
    "9: í˜íŠ¸ + ë‹¤ì¤‘í¬ì¥ì¬  \n",
    "10: ìŠ¤í‹°ë¡œí¼  \n",
    "11: ê±´ì „ì§€  \n",
    "\n",
    "í•™ìŠµ ê³¼ì •ì—ì„œ ê°€ì • ë‚´ ë¶„ë¦¬ë°°ì¶œ ê³¼ì •ê³¼ ëª¨ë¸ ì •í™•ë„ ë“±ì„ ê³ ë ¤í•˜ì—¬  \n",
    "ì´ 12ê°œì˜ í´ë˜ìŠ¤ë¡œ í†µí•©\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vit/venv/eda_venv/lib/python3.12/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.89 ğŸš€ Python-3.12.3 torch-2.6.0+cu124 CPU (11th Gen Intel Core(TM) i7-11370H 3.30GHz)\n",
      "Setup complete âœ… (8 CPUs, 23.2 GB RAM, 100.1/467.3 GB disk)\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'> 12\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# Load the model\n",
    "model = YOLO('/home/vit/dev_ws/project/Media-Project/ai_server/12_model.pt')  # load a pretrained model\n",
    "print(type(model.names),len(model.names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 0, 286.5ms\n",
      "Speed: 2.1ms preprocess, 286.5ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 0s, 1 8, 387.9ms\n",
      "Speed: 2.9ms preprocess, 387.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 0s, 1 9, 274.9ms\n",
      "Speed: 13.3ms preprocess, 274.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 0s, 1 9, 259.6ms\n",
      "Speed: 14.9ms preprocess, 259.6ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 0s, 1 9, 241.2ms\n",
      "Speed: 7.4ms preprocess, 241.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 0s, 1 9, 236.8ms\n",
      "Speed: 6.9ms preprocess, 236.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 0s, 246.8ms\n",
      "Speed: 6.3ms preprocess, 246.8ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 1 0, 232.5ms\n",
      "Speed: 5.5ms preprocess, 232.5ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 0s, 222.9ms\n",
      "Speed: 15.0ms preprocess, 222.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 0s, 249.2ms\n",
      "Speed: 8.2ms preprocess, 249.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 0s, 202.1ms\n",
      "Speed: 4.5ms preprocess, 202.1ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 0s, 273.5ms\n",
      "Speed: 8.0ms preprocess, 273.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 3 0s, 1 9, 239.4ms\n",
      "Speed: 3.4ms preprocess, 239.4ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "\n",
      "0: 480x640 2 0s, 1 9, 270.9ms\n",
      "Speed: 2.4ms preprocess, 270.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. ëª¨ë¸ ë¡œë“œ\n",
    "model = YOLO('/home/vit/dev_ws/project/Media-Project/ai_server/12_model.pt') \n",
    "\n",
    "# 2. ì›¹ìº  ì—°ê²° \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        break\n",
    "\n",
    "    # 3. ëª¨ë¸ ì¶”ë¡  ì‹œ conf=0.5 íŒŒë¼ë¯¸í„°ë¥¼ í†µí•´ 0.5 ì´ìƒì¸ ê²°ê³¼ë§Œ ë°˜ì˜\n",
    "    results = model(frame, conf=0.5)\n",
    "\n",
    "    # 4. ê²°ê³¼ ì‹œê°í™” (ì‹ ë¢°ë„ê°€ 0.5 ë¯¸ë§Œì¸ ê°ì²´ëŠ” í‘œì‹œë˜ì§€ ì•ŠìŒ)\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # 5. ê²°ê³¼ ì´ë¯¸ì§€ë¥¼ í™”ë©´ì— í‘œì‹œ\n",
    "    cv2.imshow(\"YOLO Detection\", annotated_frame)\n",
    "\n",
    "    # 'q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "         break\n",
    "\n",
    "# 6. ì›¹ìº  ë° ìœˆë„ìš° ë¦¬ì†ŒìŠ¤ í•´ì œ\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 480x640 1 0, 1 3, 289.0ms\n",
      "Speed: 1.7ms preprocess, 289.0ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 1 0, 1 10, 206.9ms\n",
      "Speed: 1.3ms preprocess, 206.9ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 1 0, 159.7ms\n",
      "Speed: 2.4ms preprocess, 159.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 1 0, 140.1ms\n",
      "Speed: 3.5ms preprocess, 140.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 1 0, 140.9ms\n",
      "Speed: 2.5ms preprocess, 140.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 1 0, 148.3ms\n",
      "Speed: 1.7ms preprocess, 148.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 (no detections), 148.2ms\n",
      "Speed: 2.5ms preprocess, 148.2ms inference, 0.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 (no detections), 141.7ms\n",
      "Speed: 2.4ms preprocess, 141.7ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 2 0s, 144.2ms\n",
      "Speed: 2.0ms preprocess, 144.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 2 0s, 136.8ms\n",
      "Speed: 3.0ms preprocess, 136.8ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 2 0s, 1 9, 149.5ms\n",
      "Speed: 4.9ms preprocess, 149.5ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 1 0, 142.5ms\n",
      "Speed: 1.6ms preprocess, 142.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 1 0, 169.4ms\n",
      "Speed: 9.7ms preprocess, 169.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 1 0, 138.5ms\n",
      "Speed: 2.3ms preprocess, 138.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 1 0, 150.7ms\n",
      "Speed: 1.9ms preprocess, 150.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n",
      "\n",
      "0: 480x640 1 0, 169.4ms\n",
      "Speed: 6.0ms preprocess, 169.4ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# 1. ëª¨ë¸ ë¡œë“œ\n",
    "model = YOLO('/home/vit/dev_ws/project/deepcycle_project/src/12_model.pt')\n",
    "\n",
    "# 2. ì›¹ìº  ì—°ê²° \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"ì›¹ìº ì„ ì—´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        break\n",
    "\n",
    "    # 3. ì›¹ìº ì˜ ì›ë³¸ í”„ë ˆì„ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œ (ê²€ì¶œ ê²°ê³¼ëŠ” ì•„ë‹˜)\n",
    "    cv2.imshow(\"Webcam Feed\", frame)\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    if key == ord('c'):\n",
    "        # 'c' í‚¤ë¥¼ ëˆ„ë¥´ë©´ í˜„ì¬ í”„ë ˆì„ ìº¡ì³ í›„ ëª¨ë¸ ì¶”ë¡  ì§„í–‰\n",
    "        results = model(frame, conf=0.5)\n",
    "        annotated_frame = results[0].plot()\n",
    "        \n",
    "        # ìº¡ì³ëœ ì´ë¯¸ì§€ì˜ íƒì§€ ê²°ê³¼ë¥¼ ë³„ë„ì˜ ì°½ì— í‘œì‹œ\n",
    "        cv2.imshow(\"YOLO Detection\", annotated_frame)\n",
    "        print(\"íƒì§€ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤. ì°½ì„ ë‹«ìœ¼ë ¤ë©´ ì•„ë¬´ í‚¤ë‚˜ ëˆ„ë¥´ì„¸ìš”.\")\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyWindow(\"YOLO Detection\")\n",
    "    \n",
    "    elif key == ord('q'):\n",
    "        # 'q' í‚¤ë¥¼ ëˆ„ë¥´ë©´ ì¢…ë£Œ\n",
    "        break\n",
    "\n",
    "# 4. ì›¹ìº  ë° ìœˆë„ìš° ë¦¬ì†ŒìŠ¤ í•´ì œ\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eda_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
